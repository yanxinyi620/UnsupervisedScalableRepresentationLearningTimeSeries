3. Unsupervised Training

我们寻求训练一个仅有编码器的架构，避免像Malhotra等人（2017）所做的基于自动编码器的标准表示学习方法那样，需要与解码器联合训练，因为这些会引起较大的计算成本。为此，我们引入了一种新颖的时间序列的三倍频损失，其灵感来自于被称为word2vec(Mikolov等人，2013)的成功且现在已经很经典的词表示学习方法。所提出的三重损失使用原始的基于时间的采样策略来克服在未标记数据上学习的挑战。据我们所知，这项工作是时间序列文献中第一个在完全无监督环境下依赖三倍子损失的工作。

其目标是确保相似的时间序列获得相似的表示，而不需要监督来学习这种相似性。三重损失有助于实现前者（Schroff等，2015），但需要提供成对的相似输入，因此对后者提出了挑战。以往使用三重损失的时间序列的监督作品都假设数据是有标注的，而我们引入了一种无监督的基于时间的标准来选择相似的时间序列对，并考虑到不同长度的时间序列，遵循word2vec的直觉。word2vec的CBOW模型所做的假设有两个方面。一方面，一个词的上下文的表示可能应该与这个词的上下文接近（Goldberg & Levy，2014），另一方面，与随机选择的词的上下文保持距离，因为它们可能与原词的上下文无关。相应的损失就会促使（上下文，单词）和（上下文，随机单词）的对子是可以线性分离的。这就是所谓的负采样。

为了使这一原则适应时间序列，我们考虑（见图1的说明）一个给定时间序列yi的随机子序列2 x ref。那么，一方面，x ref的表示应该接近其任何一个子序列x pos的表示（一个正例）。另一方面，如果我们考虑随机选择的另一个子序列x neg（一个负面的例子）（在不同的随机时间序列yj中，如果有几个序列可供选择，或者在同一时间序列中，如果它足够长并且不是静止的），那么它的表示应该与x ref的表示相距甚远。按照word2vec的类比，x pos对应一个词，x ref对应其上下文，x neg对应一个随机词。为了提高训练过程的稳定性和收敛性，以及我们学习的表征的实验结果，我们像word2vec一样，引入了几个负样本(x neg k)k∈J1,KK ，独立随机选择。

在训练过程中，这些选择对应的目标要最小化，可以认为是word2vec的目标，它的浅层网络被一个参数为θ的深层网络f(-，θ)所取代，或者形式上，σ是sigmoid函数。这种损失促使计算出的表示法区分x ref和x neg，并同化x ref和x pos。总的来说，训练过程包括在训练数据集上进行多次循环（可能使用迷你批次），按照算法1中的详细说明，随机选取元组x ref、x pos、（x neg k）k，并对每对元组的相应损失进行最小化步骤，直到训练结束。整体的计算和内存成本为O(K - c(f))，其中c(f)是在时间序列上通过f进行评估和反向传播的成本；因此，只要编码器架构也是可扩展的，这种无监督训练就是可扩展的。在算法1中，负例的长度是在最一般的情况下随机选择的；但是，它们的长度也可以对所有样本都是一样的，并且等于size（x pos）。) 后一种情况适用于数据集中所有时间序列长度相等的情况，由于计算因子化，加快了训练过程的速度；前一种情况只用于数据集中时间序列长度不一样的情况，因为我们实验中看到两种情况除了时间效率外没有其他区别。在我们的实验中，我们不对x ref、x pos和x的长度进行上限。neg，因为它们已经被火车时间序列的长度所限制，这对应于我们的表征所测试的长度尺度。

Algorithm 1: 
Choices of x ref, x pos and (x neg k) k ∈J1, KK for an epoch over the set (yi) i∈J1,NK.

我们强调，这种基于时间的三重损失利用了所选编码器的能力，可以将不同长度的时间序列作为输入。通过对编码器进行从1到训练集中最长的时间序列长度范围的训练，它能够输出有意义的和可转移的表示，而不管输入长度如何，如第5节所示。

这个训练过程的有趣之处在于，它足够高效，可以在长时间序列上运行。(见第5节)和可扩展的编码器(见第4节)，这得益于它的无解码器设计和可扩展的编码器。损失的可分离性，在此基础上可以进行每期的反向传播以节省内存。

通过www.DeepL.com/Translator（免费版）翻译
